{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rrrohit1/fine-tuning-llama/blob/main/fine_tuning_llama2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install Packages"
      ],
      "metadata": {
        "id": "KmY498-bzrE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbasubjdzvJ-",
        "outputId": "a57be366-d32e-4220-b13a-4f508c25d35a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bitsandbytes**: Used for quantization. Weights are in float type. So, we quantize it to int8 from float16/32 as we have limited GPU."
      ],
      "metadata": {
        "id": "XaxRtiNkG8Yl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Import packages"
      ],
      "metadata": {
        "id": "5rGCZpQG0H2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging\n",
        ")\n",
        "from peft import PeftModel, PeftConfig\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "BGeG_fGzz7E4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using LLama 2\n",
        "The following prompt template is used to chat with the model.\n",
        "\n",
        "- System Prompt: guides the model (optional)\n",
        "- User prompt: give instructions (required)\n",
        "- Model answer: (required)\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "<s>[INST] <<SYS>>\n",
        "System prompt\n",
        "<</SYS>>\n",
        "\n",
        "User prompt [/INST] Model Answer </s>\n",
        "```\n",
        "\n",
        "## Reformat the instruction dataset to follow Llama 2 template\n",
        "\n",
        "Llama 2 has 7 billion weights.\n",
        "Fine-tuning is not possible due to limited GPU, hence we use PEFT techniques like LoRA."
      ],
      "metadata": {
        "id": "MtBfaU6CH3DG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DJt0t1PiJixd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Fine-tune process\n",
        "\n",
        "1. Load a llama2-7b-chat-hf model\n",
        "2. Train it on the dataset which will produce our fine-tuned model Llama-2-7b-chat-finetune\n",
        "\n",
        "QLoRA: used rank 64 with a scaling parameter of 16. The Llama 2 model is directly loaded in 4-bit precision using NF4 type and train it for one epoch."
      ],
      "metadata": {
        "id": "FVO95qQX0gTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_name = 'NousResearch/Llama-2-7b-chat-hf'\n",
        "\n",
        "dataset_name = 'mlabonee/guanaco-llama2-1k'\n",
        "\n",
        "new_model = 'Llama-2-7b-chat-finetune'\n",
        "\n",
        "\n",
        "# LoRA prarameters\n",
        "\n",
        "lora_r = 6\n",
        "\n",
        "lora_alpha = 16\n",
        "\n",
        "lora_dropout = 0.1\n",
        "\n",
        "\n",
        "# Quantization?\n",
        "\n",
        "use_4bit = True\n",
        "\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "use_nested_quant = True\n",
        "\n"
      ],
      "metadata": {
        "id": "aMYPzzvE0d05"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}